---
title: Coursera机器学习讲义（二）：单元线性回归
date: 2017-01-14 21:44:32
tags: [线性回归,梯度下降,机器学习,Coursera]
categories: [Coursera机器学习讲义]
---

## 问题定义

给定如下的数据集：

![](datas.png)

我们想要根据住房的尺寸来预估房价：

![](problem.png)

## 模型描述

为了以后方便描述，我们用$x^{(i)}​$表示输入变量，也叫输入特征，$y^{(i)}​$表示输出，或者我们想要预测的目标变量。$(x^{(i)},y^{(i)})​$叫做训练样本，我们用来学习的数据集——一系列的训练样本$(x^{(i)},y^{(i)});i=1,...,m​$——叫做训练集。注意以上出现的上标“$(i)​$”仅仅是训练样本在训练集中的索引编号，不是幂运算。用大写$X​$表示输入空间，$Y​$表示输出空间。

更正式一点地描述监督学习，我们的目标是，对于给定的训练集，习得一个函数$h:X→Y$使得$h(x)$可以很好地预测输入$x$所对应的$y$值。这个函数$h$被习惯性地称作一个**假设** *(hyphothesis)*，在该问题中我们的假设为：			
$$
h_{\theta}(x^{(i)})= \theta_{0} + \theta_{1} x^{(i)}
$$
整个流程如下图所示:

![](model-representation.png)

当我们要预测的目标变量是连续的时(如这里需要预测的房价)，我们称这种学习问题为回归问题。

## 代价函数

我们可以定义***代价函数*** *(Cost Function)*用以衡量假设函数的准确性：

$$
J(\theta_{0},\theta_{1})={1\over2m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^{2}
$$
即为训练集中训练样本假设预测值与实际目标结果之差平方的平均数$×{1\over2}$（此处的$×{1\over2}$是为了方便后续梯度下降计算方便）。

我们的学习算法要做的就是选择参数$\theta_0,\theta_1$, 使得$h_{\theta}(x)$与训练样本的实际结果$y$尽可能相近，即使得代价函数$J$最小。

### 直观印象

左右分别为该问题中代价函数$J$的三维图像和等值线图。

![](cost-intuition.png)

以上右图中标记处所对应的$\theta$值为参数的假设直线如下：

![](predict-line.png)

## 梯度下降

现在有了假设函数和用来估计假设函数中各个参数的代价函数，现在要做的就是通过学习算法得到最优的参数——梯度下降算法。注意到代价函数关于假设函数中参数$\theta$的函数，把$\theta_0$放到x轴，$\theta_1$放到y轴，代价函数为z轴，得到代价函数的图像：

![](cost-graph.png)

梯度下降算法所做的，就是在随机选定了参数$\theta_0,\theta_1$初始值的情况下，沿着代价函数下降最快的方向一步一步改变$\theta_0,\theta_1$，直到代价函数不再变化，每步的大小取决于算法的参数$\alpha$，方向则取决于代价函数在该处的偏导数。如同图中从红圈中的初始点沿着代价函数的"山坡"一步步向下走，直到谷底。

需要注意的是，梯度下降所得到的可能只是局部最优解，这取决于选取的初始值。即最终是会到达一个谷底，但是可能在其他的位置还会有更低的谷底。分别上图中两个不同的初始值，梯度下降算法可能就会得到不同的结果。

repeat:
$$
\theta_{j}:=\theta_{j}-\alpha{\partial\over\partial\theta_{j}}J(\theta_0,\theta_1)
$$
where j=0,1 represents the feature index number,

until converge.

### 同步更新

**★注意：**每次迭代必须确保同步更新各个参数$\theta_1,\theta_2,...,\theta_n$。在每次迭代中如果先更新某个参数之后再更新剩余的参数会影响最终的结果。![](simutaneously-update.png)

### 步长的取值

步长参数$\alpha$的取值也会影响算法的执行。如果步长设置得过小，会导致算法收敛得非常慢；如果步长设置过大的话，可能会导致无法收敛。

![](setting-alpha.png)

### 应用于单元线性回归

当在单元线性回归中应用梯度下降算法时，主要就是在每次迭代中求出线性回归的代价函数对于每个参数$\theta_1,\theta_2,...,\theta_n$的偏导：

![](partial-derivative.png)

则对于单元线性回归，梯度下降算法的形式为：

 repeat:
$$
\begin{split}\theta_0& := \theta_0 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m}((h_\theta(x^{(i)}) - y^{(i)}) \\
\theta_1& := \theta_1 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m}((h_\theta(x^{(i)}) - y^{(i)}) x^{(i)})\end{split}
$$
until converge.

主要的思想就是为假设方程的参数猜测一组初始值，然后随着不断地用梯度下降进行迭代，我们的假设方程会变得越来越准确。